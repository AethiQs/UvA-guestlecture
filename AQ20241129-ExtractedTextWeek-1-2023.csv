;dc_title;page;content
0;MLE_Week_1.pdf;0;"```
University of Amsterdam
Amsterdam School of Economics

Machine Learning for Econometrics

Lecture 1: Statistical Learning Framework

Yi He
October 31, 2023
```"
1;MLE_Week_1.pdf;1;"Plan for Today

1. Course Introduction

2. Motivating Example: Support Vector Machines

3. Beyond SVM: Statistical Learning Framework

4. Unifying Classification and Regression

Â©UvA                                                                   1"
2;MLE_Week_1.pdf;2;Course Introduction
3;MLE_Week_1.pdf;3;"ML for Econometrics 2023â€“2024

Our team:

Yi He (Associate Professor)             Ming Cheng (PhD Candidate)

- Tuesday lectures and Thursday tutorials
- Submit pre-class questions by every Wednesday noon.
- Computer labs by my colleague Ming

Â©UvA                                                                                                                                        2"
4;MLE_Week_1.pdf;4;"What is Machine Learning ... for Economists?

Athey S. (2018), â€œThe Impact of Machine Learning on Economicsâ€:
   ..., machine learning is a field that develops algorithms designed to be applied to datasets, with the main areas of focus being prediction [regression], classification, and clustering or grouping tasks.

   * Statistical Learning Framework: Week 1
   * Deep Learning: Week 2
   * Guest Lecture (Text Analytics): Week 3
   * Boosting: Week 4
   * Random Forest: Week 5
   * Causal inference: Week 6
   
@UvA                                                                                                                                                                                                                                                                                                                                                                                                                   
3"
5;MLE_Week_1.pdf;5;"ML for Econometrics 2023â€“2024: Assessment

â€¢ Computer assignments: 20% + 20% = 40%
â€¢ Final exam, 2 hours, 5.5+ to pass: 60%
â€¢ Form assignment groups voluntarily until Tue 7 Nov via Canvas
â€¢ Max 3 students in each group

@UvA                                                                                                                                  4"
6;MLE_Week_1.pdf;6;"Learning Materials

Our main learning materials are in the online reader:
https://machinelearningtheory.org/

More reference materials:

(Pattern recognition and machine learning). Christopher M. Bishop (Title and Author extracted by OCR from image).
(Deep Learning). Ian Goodfellow and Yoshua Bengio. (Title and Author extracted by OCR from image).
(Computer Age Statistical Inference). Bradley Efron and Trevor Hastie. (Title and Author extracted by OCR from image).

â€¢ Links for free online reading of the reference books on Canvas

@UvA    5"
7;MLE_Week_1.pdf;7;Motivating Example: Support Vector Machines
8;MLE_Week_1.pdf;8;"```
Optimal Separating Hyperplane

                         S = {Y_i, X_i : i = 1, ..., n},
                               X_i = (X_i1, ..., X_id)^T âˆˆ â„^d, Y_i âˆˆ {-1, +1}



Figure 19.1 of Efron and Hastie (2016): linear discriminant function f(x) =
w^T x + w_0, decision rule C(x) = sign(f(x))

Â©UvA                                                                         6
```"
9;MLE_Week_1.pdf;9;"Signed Distances

    Signed distances from points \( X_i \) to the decision boundary

    \(\{ x : w^T x + w_0 = 0, \|w\| = 1 \} \)

    are given by:
    
    \[
    w^T X_i + w_0 =
    \begin{cases}
    >0 & \Rightarrow C(X_i) = 1 \\
    =0 & \text{ on the boundary, } i = 1, \ldots, n \\
    <0 & \Rightarrow C(X_i) = -1
    \end{cases}
    \]

    Signed distances taking into account the classification errors:

    \[
    Y_i \cdot (w^T X_i + w_0) =
    \begin{cases}
    >0 & \Rightarrow C(X_i) = Y_i \\
    =0 & \text{ on the boundary, } i = 1, \ldots, n \\
    <0 & \Rightarrow C(X_i) \neq Y_i
    \end{cases}
    \]

@UvA"
10;MLE_Week_1.pdf;10;"```
Hard-Margin Support Vector Machine

Assuming separability, maximizing the margins gives hard-margin
SVM [Tute Q1]:

     maximize M
   w,w_0,M

subject to    Y_i Â· (w^T X_i + w_0) â‰¥ M âˆ€ i    â‡”    minimize 1/2 Î²^T Î²
                                               Î²,Î²_0

             â°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Beyond Marginâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â°                   subject to    Y_i(Î²^T X_i + Î²_0) â‰¥ 1 âˆ€ i

        â€–wâ€– = 1

        ... in the sense that they give the same decision boundary

        {x : Åµ^T x + Åµ_0 = 0} = {x : Î²Ì‚^T x + Î²Ì‚_0 = 0}

@UvA
```"
11;MLE_Week_1.pdf;11;"```
Soft-Margin Support Vector Machine

The soft-margin SVM, with some hyperparameter C > 0:

        minimize         1/2  Î²^T Î² + C âˆ‘(i=1 to n) Î¾_i
       Î², Î²_0, {Î¾_i}

subject to   Y_i(Î²^T X_i + Î²_0)  â‰¥  1 - Î¾_i       âˆ€i
                                       â±
                Violating Margins    Î¾_i  â‰¥  0    âˆ€i
                                       â°

Sending C â†’ âˆ gives the hard-margin version.

For each i, the constraints are
             â§                     â«
             âª Î¾_i â‰¥ 1 - Y_i(Î²^T X_i + Î²_0) âª     âŸº  Î¾_i â‰¥ max{1 - Y_i(Î²^T X_i + Î²_0), 0}
             â¨                            â¬
             âª         Î¾_i  â‰¥ 0           âª
             â©                            â­

@UvA                                             9
```"
12;MLE_Week_1.pdf;12;"```
Alternative Formulation of Soft-Margin SVM

Minimizing Î¾_i for all i gives that
 Î¾_i = max{1 - Y_i(Î²^T X_i + Î²_0), 0}

Plugging in the objective function yields a unconstrained problem:
              n
minimize Â½Î²^T Î² + C âˆ‘ max{1 - Y_i(Î²^T X_i + Î²_0), 0}
   Î², Î²_0           i=1          Î¾_i

Dividing the objective function by the constant nC does NOT change the solution of Î², Î²_0

               n
minimize Î»Î²^T Î² + Â¹/â‚™ âˆ‘ max{1 - Y_i(Î²^T X_i + Î²_0), 0}
    Î², Î²_0              i=1

with Î» = Â¹/2nC > 0.

@UvA                                                                                                 10
```"
13;MLE_Week_1.pdf;13;"Hinge Loss

The objective function
                Î»Î²áµ€Î² +  1/n  Î£   max{1 - Yáµ¢ ( Î²áµ€Xáµ¢ + Î²â‚€ ), 0}
                      i=1        f(Xáµ¢;Î²,Î²â‚€)

                =Î»Î²áµ€Î² +  1/n  Î£   â„“â‚•( f(Xáµ¢ ; Î², Î²â‚€), Yáµ¢)
                              i=1

where â„“â‚• is called the hinge loss function given by

                â„“â‚•(f(x), y) = max{1 - y â‹… f(x), 0}

and f(x; Î², Î²â‚€) is a candidate linear prediction rule in form of

                f(x; Î², Î²â‚€) = Î²áµ€x + Î²â‚€

Â©UvA  11"
14;MLE_Week_1.pdf;14;"Regularized Estimation

The Lagrange multiplier theorem states that:

minimize
\[
\beta, \beta_0 \quad \lambda \beta^T \beta + \frac{1}{n} \sum_{i=1}^{n} \ell_H(f(X_i; \beta, \beta_0), Y_i) \tag{1}
\]

is equivalent to

minimize
\[
\beta, \beta_0 \quad \frac{1}{n} \sum_{i=1}^{n} \ell_H(f(X_i; \beta, \beta_0), Y_i) \quad
\begin{array}{c}
\underbracket[0.5pt]{\text{Empirical Risk}}
\end{array}
\]

subject to \(\beta^T \beta \leq b\) \(\Rightarrow\) Regularization


where \(b = \hat{\beta}^T \hat{\beta}\) where \(\hat{\beta}\) is the solution of (1).

@UvA

12"
15;MLE_Week_1.pdf;15;"Empirical Risk

The empirical hinge risk (= average hinge loss)

L_S(f) = \frac{1}{n} \sum_{i=1}^{n} \ell_H(f(X_i; \boldsymbol{\beta}, \beta_0), Y_i)

is an estimator of the population hinge risk (= expected hinge loss)

L_D(f) = \mathbb{E}L_S(f) = \mathbb{E}[\ell_H(f(X; \boldsymbol{\beta}, \beta_0), Y)]

if Y_i, X_i are i.i.d. observations from the distribution \mathcal{D} of \mathbb{Y}, \mathbb{X}.

 â€¢ The regularization improves statistical performance: we will come back to this point later

@UvA 13"
16;MLE_Week_1.pdf;16;"SVM: What Does It Estimate?

Now consider a general, not necessarily linear, candidate prediction rule \( f : \mathbb{R}^d \rightarrow \mathbb{R} \) with the hinge risk
\[ L_D(f) = \mathbb{E}[\ell_H(f(X), Y)] = \mathbb{E}[\max \{0, 1 - Y \cdot f(X)\}]. \]

The ideal prediction rule \( f^* \) minimizing this risk function is
\[ f^*(x) = \text{sign} \left( \mathbb{E}[Y | X = x] \right) \]
\[ = \begin{cases} 
1 & \mathbb{P}(Y = 1 | X = x) > \mathbb{P}(Y = -1 | X = x) \\
-1 & \mathbb{P}(Y = 1 | X = x) < \mathbb{P}(Y = -1 | X = x) 
\end{cases} \]
\[ =: C_{\text{Bayes}}(x) \]

\( C_{\text{Bayes}} \) is called the Bayes classifier.

In other words, the soft-margin SVM is directly estimating the Bayes classifier \( C_{\text{Bayes}}(x) \).

@UvA 14"
17;MLE_Week_1.pdf;17;"Why Bayes Classifier?

The Bayes classifier C^(Bayes)(x) *minimizes* the probability of making classification mistakes:

         minimize(_g) ğ’«(g(X) â‰  Y) = minimize(_g) ğ¸[â„“_(0-1)(g(X), Y)]

over the class of classifier g : â„^d â†’ {-1, +1}, where

                     â„“_(0-1)(Å·, y) = 1[Å· â‰  y]

is called the zero-one loss function.

@UvA                                                  15"
18;MLE_Week_1.pdf;18;"Remark: Linear Rule Is Not Enough

Recall that the soft-margin SVM is directly estimating the Bayes classifier C^(Bayes)(x) âˆˆ {âˆ’1, 1}, which is nonlinear. However, the standard soft-margin SVM uses only linear prediction rules

f(x) = Î²^(T)x + Î²_0.

That's why we need to use the kernel tricks (not to be discussed) to generate a richer space

ğ“• = { f : f(x) = Î£_(i=1) ^(n) Î±_i K(x, x_i) },

where K âˆˆ ğ“š is a kernel function so we can approximate C^(Bayes)(x) with || C^(Bayes)(x) - f*(x) ||_(K) < Ïµ for a (arbitrarily) small Ïµ > 0 for some f* âˆˆ ğ“•.

@UvA 16"
19;MLE_Week_1.pdf;19;"```
Regularization: Variance Reduction

Recall that we only optimize the empirical risk over a smaller
subset in SVM given by

              ğ’¢ = {ğ‘“(ğ‘¥) = Î²áµ€ğ‘¥ + Î²â‚€ âˆˆ ğ“• : â€–Î²â€–Â² â‰¤ b}

and our estimator is given by

                        ğ‘“Ì‚ = argmin ğ‘³â‚›(ğ‘“).
                            ğ‘“âˆˆğ’¢

The motivation is to reduce variance of ğ‘“Ì‚ due to the randomness
of ğ‘Œáµ¢, ğ‘‹áµ¢, which increases in general with the worst estimation error
of the risk function:

             sup |ğ‘³â‚›(ğ‘“) âˆ’ ğ‘³á´°(ğ‘“)|  â†‘  as  ğ’¢  â†‘ .
             ğ‘“âˆˆğ’¢

@UvA                                                      17
```"
20;MLE_Week_1.pdf;20;"Regularization: Bias Variance Tradeoff

However, shrinking the space ğ“— restricts our choices of candidate prediction rules. The best population risk we can reach is only

          inf          L_D(f)
          fâˆˆğ“—

so the bias

          inf           L_D(f)  â€”  inf          L_D(f)  â†‘  as ğ“— â†“.
          fâˆˆğ“—                            fâˆˆâ„±

In practice, we need to trade-off bias and variance by tuning the hyperparameter Î» (and hence b) properly.

@UvA                                                                                                              18"
21;MLE_Week_1.pdf;21;Beyond SVM: Statistical Learning Framework
22;MLE_Week_1.pdf;22;"Statistical Learning Problems

Target variable \( Y \in \mathcal{Y} \), \( d \) features \( X \in \mathcal{X} \subseteq \mathbb{R}^d \)

Before observing \( Y \):

We make a prediction \( f(X) \) with some prediction rule \( f : \mathcal{X} \to \mathcal{Y} \)

After observing \( Y \): We quantify the predictive loss by
\[ \ell(f(X), Y) \]

where \( \ell : \mathcal{Y} \times \mathcal{Y} \to [0, \infty) \) is some loss function specified by the user. A larger loss implies a worse prediction performance, and the loss remains non-negative such that
\[ \ell(f(X), Y) \ge \ell(Y, Y) = 0. \]

Our goal is to find the ideal prediction rule that minimizes the population risk function given by
\[ L_{\mathcal{D}}(f) = \mathbb{E}[\ell(f(X), Y)], \quad f \in \mathcal{F}. \]

@UvA
19"
23;MLE_Week_1.pdf;23;"Ideal Prediction Rule

â— The regression function

        Î¼(x) = E[Y | X = x]

    is ideal for the (half) squared loss function

        â„“(Å·, y) = (1/2) â€–Å· - yâ€–^2

â— The Bayes classifier

        C^(Bayes)(x) = argmax_(Ck,1â‰¤kâ‰¤K) P(Y = Ck | X = x)

    is ideal for the zero-one loss â„“_0-1 among classifiers and hinge loss â„“_H among all real-valued functions as discussed before.

Â©UvA 20"
24;MLE_Week_1.pdf;24;"Empirical Risk Minimization

Given a random sample

\[ S = \{Y_i, X_i : i = 1, \ldots, n\}, \]

the empirical risk function is equal to the average loss given by

\[ L_S(f) = \frac{1}{n} \sum_{i=1}^{n} \ell(f(X_i), Y_i), \quad f \in \mathcal{F}. \]

The empirical risk minimization (ERM) paradigm solves the optimization problem:

\[
\begin{aligned}
& \text{minimize} \quad L_S(f) \\
& \text{subject to} \quad f \in \mathcal{H}
\end{aligned}
\quad \Rightarrow \quad \text{Regularization}
\]

How to choose \( \mathcal{H} \subset \mathcal{F} \)?

Â©UvA 21"
25;MLE_Week_1.pdf;25;"Penalized Method
     
  * Parametrize \( f = f(X; \theta) \in \mathcal{H} \) with parameters \( \theta \in \Theta \)
  * Measure model complexity with some criterion function \( C(\theta) \)
  * Choose \( \mathcal{H} \) with a limited complexity such that
  
 \[
 \mathcal{H} = \{ f( \cdot ; \theta) \in \mathcal{F} : C(\theta) \leq b , \theta \in \Theta \}
 \]
      
 This is equivalent to minimize the penalized empirical risk function
 
 \[
 \text{minimize } L_S (f) + \lambda \cdot C(\theta)
 \]
 
 where \( \lambda = \lambda(b) \) is a tuning parameter.

  

@UvA    22"
26;MLE_Week_1.pdf;26;"Unifying Classification and Regression
------------------------------------"
27;MLE_Week_1.pdf;27;"One-Hot Encoding

â— Classification algorithms generate discrete outputs but regression algorithms generate continuous outputs.

â— There is a strong connection between these two tasks:

   C^Bayes(x) 
     classification 
     = argmax_(C_k, 1<=k<=K) P(Y = C_k | X = x) 
     = argmax_(C_k, 1<=k<=K) E[1[Y = C_k] | X = x] 
     regression 

â— (Y^(1), ..., Y^(K)) is the one-hot encoding of Y with Y^(k) = 1[Y = C_k].

â— We can estimate the multivariate regression function 
  Î¼(x) = (Î¼_1(x), ..., Î¼_K(x)),  Î¼_k(x) = E[Y^(k) | X = x] 

  using regression algorithms, and then construct the Bayes classifier.

@UvA 23"
28;MLE_Week_1.pdf;28;"Softmax Function

â€¢ However, the regression function for one-hot encoded target
  must satisfy the axioms of probability:
  
  \[
  \mu(x) \in \mathcal{P}_k = \left\{ y \in (0, 1)^K : \sum_{k=1}^{K} y_k = 1 \right\}, \quad \forall x \in \mathcal{X}.
  \]

â€¢ Re-parametrize the candidate regression function
  \(f(x) = (f_1(x), \ldots, f_K(x)) \in \mathcal{P}_k\) jointly by

  \[
  f(x) = \begin{pmatrix} f_1(x) \\ \vdots \\ f_K(x) \end{pmatrix} = \sigma \left( \begin{pmatrix} a_1(x) \\ \vdots \\ a_K(x) \end{pmatrix} \right) = \sigma(a(x))
  \]

  \[
  \sigma(a) = \begin{pmatrix} \sigma(a)_1 \\ \vdots \\ \sigma(a)_K \end{pmatrix}, \quad \sigma(a)_k = \frac{\exp(a_k)}{\sum_{j=1}^{K} \exp(a_j)}.
  \]

â€¢ Note that \(a(x) \in \mathbb{R}^K\) but set \(a_K(x) \equiv 0\) for identification.

Â©UvA                                             24"
29;MLE_Week_1.pdf;29;"Cross Entropy

- Note that ( Y^(1), ..., Y^(K) ) âˆˆ P_K is a distribution
- Candidate regression function ( f_1(x), ..., f_K(x) ) âˆˆ P_K is another distribution
- The cross-entropy loss function â„“_CE : P_K Ã— P_K â†’ [0, âˆ) given by

  â„“_CE(f(x), y) = Î£_(k=1)^K -y_k Â· log f_k(x)

  measures the â€˜distanceâ€™ between these two distributions
- Relax the definition to allow for y_k âˆˆ {0, 1} in algorithms
- We can show that the true regression function Î¼(x) for the one-hot encoded target is the ideal prediction rule.
- It is more specialized than the Euclidean distance induced by the squared loss

  â„“_2(f(x), y) = 1/2 || f(x) - y ||^2.


@UvA 25"
30;MLE_Week_1.pdf;30;"Unified View of Regression and Classification


  â€¢ After one-hot encoding, a classification task translates into a 
    regression task.

  â€¢ One may use soft-max transformation to enforce the axioms 
    of probability, and the cross-entropy loss instead of the 
    squared loss to estimate the multivariate regression function.

  â€¢ Therefore, other ML methods in this course are primarily for 
    regression tasks but apply to classification tasks.

  â€¢ The main differences are their choices of candidate prediction 
    rules, loss function, and regularization procedures.



@UvA									26"
