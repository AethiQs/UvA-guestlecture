;dc_title;page;content
0;MLE_Week_1.pdf;0;"```
University of Amsterdam
Amsterdam School of Economics

Machine Learning for Econometrics

Lecture 1: Statistical Learning Framework

Yi He
October 31, 2023
```"
1;MLE_Week_1.pdf;1;"Plan for Today

1. Course Introduction

2. Motivating Example: Support Vector Machines

3. Beyond SVM: Statistical Learning Framework

4. Unifying Classification and Regression

©UvA                                                                   1"
2;MLE_Week_1.pdf;2;Course Introduction
3;MLE_Week_1.pdf;3;"ML for Econometrics 2023–2024

Our team:

Yi He (Associate Professor)             Ming Cheng (PhD Candidate)

- Tuesday lectures and Thursday tutorials
- Submit pre-class questions by every Wednesday noon.
- Computer labs by my colleague Ming

©UvA                                                                                                                                        2"
4;MLE_Week_1.pdf;4;"What is Machine Learning ... for Economists?

Athey S. (2018), “The Impact of Machine Learning on Economics”:
   ..., machine learning is a field that develops algorithms designed to be applied to datasets, with the main areas of focus being prediction [regression], classification, and clustering or grouping tasks.

   * Statistical Learning Framework: Week 1
   * Deep Learning: Week 2
   * Guest Lecture (Text Analytics): Week 3
   * Boosting: Week 4
   * Random Forest: Week 5
   * Causal inference: Week 6
   
@UvA                                                                                                                                                                                                                                                                                                                                                                                                                   
3"
5;MLE_Week_1.pdf;5;"ML for Econometrics 2023–2024: Assessment

• Computer assignments: 20% + 20% = 40%
• Final exam, 2 hours, 5.5+ to pass: 60%
• Form assignment groups voluntarily until Tue 7 Nov via Canvas
• Max 3 students in each group

@UvA                                                                                                                                  4"
6;MLE_Week_1.pdf;6;"Learning Materials

Our main learning materials are in the online reader:
https://machinelearningtheory.org/

More reference materials:

(Pattern recognition and machine learning). Christopher M. Bishop (Title and Author extracted by OCR from image).
(Deep Learning). Ian Goodfellow and Yoshua Bengio. (Title and Author extracted by OCR from image).
(Computer Age Statistical Inference). Bradley Efron and Trevor Hastie. (Title and Author extracted by OCR from image).

• Links for free online reading of the reference books on Canvas

@UvA    5"
7;MLE_Week_1.pdf;7;Motivating Example: Support Vector Machines
8;MLE_Week_1.pdf;8;"```
Optimal Separating Hyperplane

                         S = {Y_i, X_i : i = 1, ..., n},
                               X_i = (X_i1, ..., X_id)^T ∈ ℝ^d, Y_i ∈ {-1, +1}



Figure 19.1 of Efron and Hastie (2016): linear discriminant function f(x) =
w^T x + w_0, decision rule C(x) = sign(f(x))

©UvA                                                                         6
```"
9;MLE_Week_1.pdf;9;"Signed Distances

    Signed distances from points \( X_i \) to the decision boundary

    \(\{ x : w^T x + w_0 = 0, \|w\| = 1 \} \)

    are given by:
    
    \[
    w^T X_i + w_0 =
    \begin{cases}
    >0 & \Rightarrow C(X_i) = 1 \\
    =0 & \text{ on the boundary, } i = 1, \ldots, n \\
    <0 & \Rightarrow C(X_i) = -1
    \end{cases}
    \]

    Signed distances taking into account the classification errors:

    \[
    Y_i \cdot (w^T X_i + w_0) =
    \begin{cases}
    >0 & \Rightarrow C(X_i) = Y_i \\
    =0 & \text{ on the boundary, } i = 1, \ldots, n \\
    <0 & \Rightarrow C(X_i) \neq Y_i
    \end{cases}
    \]

@UvA"
10;MLE_Week_1.pdf;10;"```
Hard-Margin Support Vector Machine

Assuming separability, maximizing the margins gives hard-margin
SVM [Tute Q1]:

     maximize M
   w,w_0,M

subject to    Y_i · (w^T X_i + w_0) ≥ M ∀ i    ⇔    minimize 1/2 β^T β
                                               β,β_0

             ⎰──────────────Beyond Margin──────────────⎰                   subject to    Y_i(β^T X_i + β_0) ≥ 1 ∀ i

        ‖w‖ = 1

        ... in the sense that they give the same decision boundary

        {x : ŵ^T x + ŵ_0 = 0} = {x : β̂^T x + β̂_0 = 0}

@UvA
```"
11;MLE_Week_1.pdf;11;"```
Soft-Margin Support Vector Machine

The soft-margin SVM, with some hyperparameter C > 0:

        minimize         1/2  β^T β + C ∑(i=1 to n) ξ_i
       β, β_0, {ξ_i}

subject to   Y_i(β^T X_i + β_0)  ≥  1 - ξ_i       ∀i
                                       ⎱
                Violating Margins    ξ_i  ≥  0    ∀i
                                       ⎰

Sending C → ∞ gives the hard-margin version.

For each i, the constraints are
             ⎧                     ⎫
             ⎪ ξ_i ≥ 1 - Y_i(β^T X_i + β_0) ⎪     ⟺  ξ_i ≥ max{1 - Y_i(β^T X_i + β_0), 0}
             ⎨                            ⎬
             ⎪         ξ_i  ≥ 0           ⎪
             ⎩                            ⎭

@UvA                                             9
```"
12;MLE_Week_1.pdf;12;"```
Alternative Formulation of Soft-Margin SVM

Minimizing ξ_i for all i gives that
 ξ_i = max{1 - Y_i(β^T X_i + β_0), 0}

Plugging in the objective function yields a unconstrained problem:
              n
minimize ½β^T β + C ∑ max{1 - Y_i(β^T X_i + β_0), 0}
   β, β_0           i=1          ξ_i

Dividing the objective function by the constant nC does NOT change the solution of β, β_0

               n
minimize λβ^T β + ¹/ₙ ∑ max{1 - Y_i(β^T X_i + β_0), 0}
    β, β_0              i=1

with λ = ¹/2nC > 0.

@UvA                                                                                                 10
```"
13;MLE_Week_1.pdf;13;"Hinge Loss

The objective function
                λβᵀβ +  1/n  Σ   max{1 - Yᵢ ( βᵀXᵢ + β₀ ), 0}
                      i=1        f(Xᵢ;β,β₀)

                =λβᵀβ +  1/n  Σ   ℓₕ( f(Xᵢ ; β, β₀), Yᵢ)
                              i=1

where ℓₕ is called the hinge loss function given by

                ℓₕ(f(x), y) = max{1 - y ⋅ f(x), 0}

and f(x; β, β₀) is a candidate linear prediction rule in form of

                f(x; β, β₀) = βᵀx + β₀

©UvA  11"
14;MLE_Week_1.pdf;14;"Regularized Estimation

The Lagrange multiplier theorem states that:

minimize
\[
\beta, \beta_0 \quad \lambda \beta^T \beta + \frac{1}{n} \sum_{i=1}^{n} \ell_H(f(X_i; \beta, \beta_0), Y_i) \tag{1}
\]

is equivalent to

minimize
\[
\beta, \beta_0 \quad \frac{1}{n} \sum_{i=1}^{n} \ell_H(f(X_i; \beta, \beta_0), Y_i) \quad
\begin{array}{c}
\underbracket[0.5pt]{\text{Empirical Risk}}
\end{array}
\]

subject to \(\beta^T \beta \leq b\) \(\Rightarrow\) Regularization


where \(b = \hat{\beta}^T \hat{\beta}\) where \(\hat{\beta}\) is the solution of (1).

@UvA

12"
15;MLE_Week_1.pdf;15;"Empirical Risk

The empirical hinge risk (= average hinge loss)

L_S(f) = \frac{1}{n} \sum_{i=1}^{n} \ell_H(f(X_i; \boldsymbol{\beta}, \beta_0), Y_i)

is an estimator of the population hinge risk (= expected hinge loss)

L_D(f) = \mathbb{E}L_S(f) = \mathbb{E}[\ell_H(f(X; \boldsymbol{\beta}, \beta_0), Y)]

if Y_i, X_i are i.i.d. observations from the distribution \mathcal{D} of \mathbb{Y}, \mathbb{X}.

 • The regularization improves statistical performance: we will come back to this point later

@UvA 13"
16;MLE_Week_1.pdf;16;"SVM: What Does It Estimate?

Now consider a general, not necessarily linear, candidate prediction rule \( f : \mathbb{R}^d \rightarrow \mathbb{R} \) with the hinge risk
\[ L_D(f) = \mathbb{E}[\ell_H(f(X), Y)] = \mathbb{E}[\max \{0, 1 - Y \cdot f(X)\}]. \]

The ideal prediction rule \( f^* \) minimizing this risk function is
\[ f^*(x) = \text{sign} \left( \mathbb{E}[Y | X = x] \right) \]
\[ = \begin{cases} 
1 & \mathbb{P}(Y = 1 | X = x) > \mathbb{P}(Y = -1 | X = x) \\
-1 & \mathbb{P}(Y = 1 | X = x) < \mathbb{P}(Y = -1 | X = x) 
\end{cases} \]
\[ =: C_{\text{Bayes}}(x) \]

\( C_{\text{Bayes}} \) is called the Bayes classifier.

In other words, the soft-margin SVM is directly estimating the Bayes classifier \( C_{\text{Bayes}}(x) \).

@UvA 14"
17;MLE_Week_1.pdf;17;"Why Bayes Classifier?

The Bayes classifier C^(Bayes)(x) *minimizes* the probability of making classification mistakes:

         minimize(_g) 𝒫(g(X) ≠ Y) = minimize(_g) 𝐸[ℓ_(0-1)(g(X), Y)]

over the class of classifier g : ℝ^d → {-1, +1}, where

                     ℓ_(0-1)(ŷ, y) = 1[ŷ ≠ y]

is called the zero-one loss function.

@UvA                                                  15"
18;MLE_Week_1.pdf;18;"Remark: Linear Rule Is Not Enough

Recall that the soft-margin SVM is directly estimating the Bayes classifier C^(Bayes)(x) ∈ {−1, 1}, which is nonlinear. However, the standard soft-margin SVM uses only linear prediction rules

f(x) = β^(T)x + β_0.

That's why we need to use the kernel tricks (not to be discussed) to generate a richer space

𝓕 = { f : f(x) = Σ_(i=1) ^(n) α_i K(x, x_i) },

where K ∈ 𝓚 is a kernel function so we can approximate C^(Bayes)(x) with || C^(Bayes)(x) - f*(x) ||_(K) < ϵ for a (arbitrarily) small ϵ > 0 for some f* ∈ 𝓕.

@UvA 16"
19;MLE_Week_1.pdf;19;"```
Regularization: Variance Reduction

Recall that we only optimize the empirical risk over a smaller
subset in SVM given by

              𝒢 = {𝑓(𝑥) = βᵀ𝑥 + β₀ ∈ 𝓕 : ‖β‖² ≤ b}

and our estimator is given by

                        𝑓̂ = argmin 𝑳ₛ(𝑓).
                            𝑓∈𝒢

The motivation is to reduce variance of 𝑓̂ due to the randomness
of 𝑌ᵢ, 𝑋ᵢ, which increases in general with the worst estimation error
of the risk function:

             sup |𝑳ₛ(𝑓) − 𝑳ᴰ(𝑓)|  ↑  as  𝒢  ↑ .
             𝑓∈𝒢

@UvA                                                      17
```"
20;MLE_Week_1.pdf;20;"Regularization: Bias Variance Tradeoff

However, shrinking the space 𝓗 restricts our choices of candidate prediction rules. The best population risk we can reach is only

          inf          L_D(f)
          f∈𝓗

so the bias

          inf           L_D(f)  —  inf          L_D(f)  ↑  as 𝓗 ↓.
          f∈𝓗                            f∈ℱ

In practice, we need to trade-off bias and variance by tuning the hyperparameter λ (and hence b) properly.

@UvA                                                                                                              18"
21;MLE_Week_1.pdf;21;Beyond SVM: Statistical Learning Framework
22;MLE_Week_1.pdf;22;"Statistical Learning Problems

Target variable \( Y \in \mathcal{Y} \), \( d \) features \( X \in \mathcal{X} \subseteq \mathbb{R}^d \)

Before observing \( Y \):

We make a prediction \( f(X) \) with some prediction rule \( f : \mathcal{X} \to \mathcal{Y} \)

After observing \( Y \): We quantify the predictive loss by
\[ \ell(f(X), Y) \]

where \( \ell : \mathcal{Y} \times \mathcal{Y} \to [0, \infty) \) is some loss function specified by the user. A larger loss implies a worse prediction performance, and the loss remains non-negative such that
\[ \ell(f(X), Y) \ge \ell(Y, Y) = 0. \]

Our goal is to find the ideal prediction rule that minimizes the population risk function given by
\[ L_{\mathcal{D}}(f) = \mathbb{E}[\ell(f(X), Y)], \quad f \in \mathcal{F}. \]

@UvA
19"
23;MLE_Week_1.pdf;23;"Ideal Prediction Rule

● The regression function

        μ(x) = E[Y | X = x]

    is ideal for the (half) squared loss function

        ℓ(ŷ, y) = (1/2) ‖ŷ - y‖^2

● The Bayes classifier

        C^(Bayes)(x) = argmax_(Ck,1≤k≤K) P(Y = Ck | X = x)

    is ideal for the zero-one loss ℓ_0-1 among classifiers and hinge loss ℓ_H among all real-valued functions as discussed before.

©UvA 20"
24;MLE_Week_1.pdf;24;"Empirical Risk Minimization

Given a random sample

\[ S = \{Y_i, X_i : i = 1, \ldots, n\}, \]

the empirical risk function is equal to the average loss given by

\[ L_S(f) = \frac{1}{n} \sum_{i=1}^{n} \ell(f(X_i), Y_i), \quad f \in \mathcal{F}. \]

The empirical risk minimization (ERM) paradigm solves the optimization problem:

\[
\begin{aligned}
& \text{minimize} \quad L_S(f) \\
& \text{subject to} \quad f \in \mathcal{H}
\end{aligned}
\quad \Rightarrow \quad \text{Regularization}
\]

How to choose \( \mathcal{H} \subset \mathcal{F} \)?

©UvA 21"
25;MLE_Week_1.pdf;25;"Penalized Method
     
  * Parametrize \( f = f(X; \theta) \in \mathcal{H} \) with parameters \( \theta \in \Theta \)
  * Measure model complexity with some criterion function \( C(\theta) \)
  * Choose \( \mathcal{H} \) with a limited complexity such that
  
 \[
 \mathcal{H} = \{ f( \cdot ; \theta) \in \mathcal{F} : C(\theta) \leq b , \theta \in \Theta \}
 \]
      
 This is equivalent to minimize the penalized empirical risk function
 
 \[
 \text{minimize } L_S (f) + \lambda \cdot C(\theta)
 \]
 
 where \( \lambda = \lambda(b) \) is a tuning parameter.

  

@UvA    22"
26;MLE_Week_1.pdf;26;"Unifying Classification and Regression
------------------------------------"
27;MLE_Week_1.pdf;27;"One-Hot Encoding

● Classification algorithms generate discrete outputs but regression algorithms generate continuous outputs.

● There is a strong connection between these two tasks:

   C^Bayes(x) 
     classification 
     = argmax_(C_k, 1<=k<=K) P(Y = C_k | X = x) 
     = argmax_(C_k, 1<=k<=K) E[1[Y = C_k] | X = x] 
     regression 

● (Y^(1), ..., Y^(K)) is the one-hot encoding of Y with Y^(k) = 1[Y = C_k].

● We can estimate the multivariate regression function 
  μ(x) = (μ_1(x), ..., μ_K(x)),  μ_k(x) = E[Y^(k) | X = x] 

  using regression algorithms, and then construct the Bayes classifier.

@UvA 23"
28;MLE_Week_1.pdf;28;"Softmax Function

• However, the regression function for one-hot encoded target
  must satisfy the axioms of probability:
  
  \[
  \mu(x) \in \mathcal{P}_k = \left\{ y \in (0, 1)^K : \sum_{k=1}^{K} y_k = 1 \right\}, \quad \forall x \in \mathcal{X}.
  \]

• Re-parametrize the candidate regression function
  \(f(x) = (f_1(x), \ldots, f_K(x)) \in \mathcal{P}_k\) jointly by

  \[
  f(x) = \begin{pmatrix} f_1(x) \\ \vdots \\ f_K(x) \end{pmatrix} = \sigma \left( \begin{pmatrix} a_1(x) \\ \vdots \\ a_K(x) \end{pmatrix} \right) = \sigma(a(x))
  \]

  \[
  \sigma(a) = \begin{pmatrix} \sigma(a)_1 \\ \vdots \\ \sigma(a)_K \end{pmatrix}, \quad \sigma(a)_k = \frac{\exp(a_k)}{\sum_{j=1}^{K} \exp(a_j)}.
  \]

• Note that \(a(x) \in \mathbb{R}^K\) but set \(a_K(x) \equiv 0\) for identification.

©UvA                                             24"
29;MLE_Week_1.pdf;29;"Cross Entropy

- Note that ( Y^(1), ..., Y^(K) ) ∈ P_K is a distribution
- Candidate regression function ( f_1(x), ..., f_K(x) ) ∈ P_K is another distribution
- The cross-entropy loss function ℓ_CE : P_K × P_K → [0, ∞) given by

  ℓ_CE(f(x), y) = Σ_(k=1)^K -y_k · log f_k(x)

  measures the ‘distance’ between these two distributions
- Relax the definition to allow for y_k ∈ {0, 1} in algorithms
- We can show that the true regression function μ(x) for the one-hot encoded target is the ideal prediction rule.
- It is more specialized than the Euclidean distance induced by the squared loss

  ℓ_2(f(x), y) = 1/2 || f(x) - y ||^2.


@UvA 25"
30;MLE_Week_1.pdf;30;"Unified View of Regression and Classification


  • After one-hot encoding, a classification task translates into a 
    regression task.

  • One may use soft-max transformation to enforce the axioms 
    of probability, and the cross-entropy loss instead of the 
    squared loss to estimate the multivariate regression function.

  • Therefore, other ML methods in this course are primarily for 
    regression tasks but apply to classification tasks.

  • The main differences are their choices of candidate prediction 
    rules, loss function, and regularization procedures.



@UvA									26"
