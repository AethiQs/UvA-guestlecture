{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AethiQs Assignment - NLP\n",
    "\n",
    "In this notebook we will be applying NLP on the slides of this courses (dating from 2023).\n",
    "\n",
    "The main goal is to generate questions based on the slides of last year.\n",
    "\n",
    "The slides have been processed using NLP and have been made available to you in a CSV file.\n",
    "\n",
    "\n",
    "\n",
    "Good luck and have fun!\n",
    "\n",
    "<a href=\"https://aethiqs.nl/\">\n",
    "<img src=\"https://aethiqs.nl/wp-content/uploads/AethiQs-logo.svg\" width=\"200\" height=\"200\" />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Download required pip packages\n",
    "################################################################################\n",
    "\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Load Kaggle data\n",
    "################################################################################\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ankurzing/sentiment-analysis-for-financial-news\")\n",
    "dataset = pd.read_csv(path+\"/all-data.csv\", header=None)\n",
    "dataset.columns = [\"Sentiment\", \"Text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Embedding\n",
    "\n",
    "In this exercise we will try out different methods of embeddings a piece of text. To see the results, try it on some of the texts of the given Kaggle dataset. This dataset contains the sentiments for financial news headlines from the perspective of a retail investor.\n",
    "\n",
    "**a) Use Bag of Words to encode text.**\n",
    "\n",
    "Tip: Use Scikit-learn, see: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "**b) Use a sentence transformer to encode text. Pick a model an give arguments why you chose that one.**\n",
    "\n",
    "Tips:\n",
    "- A list of available sentence transformers can be found at: https://www.sbert.net/docs/pretrained_models.html\n",
    "- For most models on this website, a link to hugging face is found where some more details about the usage of the model can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.a)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "dataset_embedded = vectorizer.fit_transform(dataset[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Measuring textual similarity\n",
    "\n",
    "**Measure textual similarity with Euclidian and cosine distance with the different embeddings of before**\n",
    "\n",
    "Try it out on some of the Kaggle texts, try sentences of different lengths or try different vocabularies and analyse the results. Use the different embedding functions as created in exercise 1 to create embeddings!\n",
    "\n",
    "Tip: Use Scikit-learn's pairwise metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Set up a text generation pipeline\n",
    "\n",
    "In this exercise, we wil build an text generation pipeline using the langchain library in combination with models from Hugging Face.\n",
    "\n",
    "Tips:\n",
    "- See this tutorial from langchain on how to use the Hugging Face pipeline with offline models: https://python.langchain.com/docs/integrations/llms/huggingface_pipelines\n",
    "- As mentioned in the tutorial above, it is also possible to create an existing pipeline using the transformers package, and loading this pipeline into the Hugging Face pipeline. For more information on these pipelines, see: https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "- Make sure to tell the pipeline to generate text as its task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home assignment: Question Generator\n",
    "\n",
    "In this assignment, we will combine what we have build before and create our own ChatBot that can generate questions about the slides related to this course of last year!\n",
    "\n",
    "The idea is as follows:\n",
    "\n",
    "We will write a prompt, instructing the chatbot with its goal. You need to think carefully about the construction of the prompt. For example, what should the chatbot do if it doesn't know the answer to a question? Should it look into its own training data or should it reply that it is unable to formulate an answer?\n",
    "\n",
    "Then, with our prompt written, we can write a question. We will use that question to gather data from the processed slides (see \"data/AQ20241129 - Extracted text week 1 2023.csv\"). We will pass both the question and our data, in combination with the prompt, to the chatbot. The chatbot should then generate an answer for our question!\n",
    "\n",
    "Tips:\n",
    "- There are a number of ways to achieve the above described chatbot, feel free to scour the internet for tutorials and explanations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Home Assigment\n",
    "################################################################################\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
